{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script is the same as the jupyter notebook provided by you to train the embeddings model on hindi dataset.\n",
    "It works for the Bangali dataset and the only difference is that I choose around 5000 samples of the dataset half of which have label 0 and the other half have label 1 so that the statistics of the dataset match that of the hindi dataset (since we want to do fine tuning from the model trained on hindi dataset). The rest of the script is the same. Please refer to Task1_Word_Embeddings.ipynp for comments on different parts of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nastaran/anaconda3/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /opt/conda/conda-bld/pytorch_1607369981906/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-a895499a6d83>:154: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = self.logsoftmax(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss :  9.166740721550541\n",
      "1\n",
      "train loss :  7.3705848403598955\n",
      "2\n",
      "train loss :  6.718367562777754\n",
      "3\n",
      "train loss :  4.915612483370131\n",
      "4\n",
      "train loss :  2.999387532040693\n",
      "5\n",
      "train loss :  1.6871720630189646\n",
      "6\n",
      "train loss :  0.8961927515009175\n",
      "7\n",
      "train loss :  0.7458769342173701\n",
      "8\n",
      "train loss :  0.7260955395042032\n",
      "9\n",
      "train loss :  0.7102453933245894\n",
      "10\n",
      "train loss :  0.6657487331093221\n",
      "11\n",
      "train loss :  0.6286886649719183\n",
      "12\n",
      "train loss :  0.6061615451522495\n",
      "13\n",
      "train loss :  0.5956463528716046\n",
      "14\n",
      "train loss :  0.586918209341989\n",
      "15\n",
      "train loss :  0.580484431290972\n",
      "16\n",
      "train loss :  0.5727338993894882\n",
      "17\n",
      "train loss :  0.5658252329930015\n",
      "18\n",
      "train loss :  0.5593273777892624\n",
      "19\n",
      "train loss :  0.554622512364733\n",
      "20\n",
      "train loss :  0.5548610756362694\n",
      "21\n",
      "train loss :  0.5482034324735835\n",
      "22\n",
      "train loss :  0.5457333097423333\n",
      "23\n",
      "train loss :  0.542739461729492\n",
      "24\n",
      "train loss :  0.5422034060609513\n",
      "25\n",
      "train loss :  0.5377405018046282\n",
      "26\n",
      "train loss :  0.5366553042245947\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import csv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "import csv\n",
    "from utils import *\n",
    "\n",
    "\n",
    "data = open('data/bengali_hatespeech.csv')\n",
    "data = csv.reader(data, delimiter='\\t')\n",
    "tweets = [line for line in data]\n",
    "\n",
    "# ## 1.2 Data preparation (0.5 + 0.5 points)\n",
    "\n",
    "\n",
    "# TODO: implement!\n",
    "stopwords = [line[:-2] for line in open('bangali_stopwords')]\n",
    "\n",
    "def valid(word):\n",
    "    if word.startswith('@') or word.startswith('(@') or word.startswith('.@'):\n",
    "        return False\n",
    "    if word in [',', ';', ':', '.', '!', '?', '\\'', '\\\"\"', '-', '_', '/', '(', ')', '[', ']', '...', '*']:\n",
    "        return False\n",
    "    if word in stopwords:\n",
    "        return False\n",
    "    #     if word in english_stopwords:\n",
    "    #         return False\n",
    "    if word.startswith('http'):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "new_tweets = []\n",
    "labels = []\n",
    "indices = np.arange(len(tweets))\n",
    "random.shuffle(indices)\n",
    "num_0 = 0\n",
    "num_1 = 0\n",
    "# tweets = tweets[indices]\n",
    "for i in indices:\n",
    "    if num_0>2500 and num_1>2500:\n",
    "        break\n",
    "    # print(tweets[i][0].split(',')[1])\n",
    "    tweet = tweets[i][0].split(',')\n",
    "    if tweet[1] not in ['0','1']:\n",
    "        continue\n",
    "    if i==0:\n",
    "        continue\n",
    "    new_sentence = []\n",
    "    # print(tweet)\n",
    "    if tweet[1] == '0':\n",
    "        num_0 += 1\n",
    "        if num_0>2500:\n",
    "            continue\n",
    "    if tweet[1] == '1':\n",
    "        num_1 += 1\n",
    "        if num_1>2500:\n",
    "            continue\n",
    "\n",
    "    labels.append(int(tweet[1]))\n",
    "    for word in tweet[0].split():\n",
    "#         print(word)\n",
    "        if valid(word):\n",
    "            cnt = 0\n",
    "            while(word[cnt]==' '):\n",
    "                cnt += 1\n",
    "            word = word[cnt:]\n",
    "            new_sentence.append(word.lower())\n",
    "#                 print(word)\n",
    "    new_tweets.append(new_sentence)\n",
    "\n",
    "word_dict = {}\n",
    "all_words = []\n",
    "for tweet in new_tweets:\n",
    "    for word in tweet:\n",
    "        if not word in word_dict.keys():\n",
    "            all_words.append(word)\n",
    "            word_dict[word] = 1\n",
    "        else:\n",
    "            word_dict[word] += 1\n",
    "V = all_words\n",
    "\n",
    "\n",
    "# * Then, write a function ```word_to_one_hot``` that returns a one-hot encoding of an arbitrary word in the vocabulary. The size of the one-hot encoding should be ```len(v)```.\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "# TODO: implement!\n",
    "def word_to_one_hot(word):\n",
    "    one_hot = np.zeros(len(V))\n",
    "    for i, w in enumerate(V):\n",
    "        if word == w:\n",
    "            one_hot[i] = 1\n",
    "            break\n",
    "    assert (np.sum(one_hot) == 1)\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "Z = word_dict\n",
    "\n",
    "\n",
    "# print(Z)\n",
    "def sampling_prob(word):\n",
    "    return (math.sqrt(Z[word] / 0.001) + 1) * (0.001 / Z[word])\n",
    "\n",
    "\n",
    "def get_target_context(sentence):\n",
    "    context_array = []\n",
    "    for i, word in enumerate(sentence):\n",
    "        for w in range(-window_size, window_size + 1):\n",
    "            context_idx = i + w\n",
    "            if context_idx < 0 or context_idx >= len(sentence) or context_idx == i:\n",
    "                continue\n",
    "            keep = np.random.choice(2, 1, p=[1.0 - sampling_prob(word), sampling_prob(word)])\n",
    "            if keep:\n",
    "                context_array.append((word, sentence[context_idx]))\n",
    "    return context_array\n",
    "\n",
    "\n",
    "# Set hyperparameters\n",
    "window_size = 5\n",
    "embedding_size = 300\n",
    "\n",
    "# More hyperparameters\n",
    "learning_rate = 0.007\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "# Create model\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_1 = nn.Linear(len(V), embedding_size)\n",
    "        self.hidden_2 = nn.Linear(embedding_size, len(V))\n",
    "        self.logsoftmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, one_hot):\n",
    "        out = self.hidden_1(one_hot)\n",
    "        out = self.hidden_2(out)\n",
    "        out = self.logsoftmax(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# # 1.8 Loss function and optimizer (0.5 points)\n",
    "\n",
    "\n",
    "# Define optimizer and loss\n",
    "import torch\n",
    "\n",
    "my_model = Word2Vec().to(device)\n",
    "optimizer = torch.optim.Adam(my_model.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "\n",
    "\n",
    "# Define train procedure\n",
    "\n",
    "# load initial weights\n",
    "def train():\n",
    "    print(\"Training started\")\n",
    "    X = []\n",
    "    Y = []\n",
    "    for sentence in new_tweets:\n",
    "        for tuples in get_target_context(sentence):\n",
    "            X.append(word_to_one_hot(tuples[0]))\n",
    "            Y.append(int(np.where(word_to_one_hot(tuples[1]) == 1)[0][0]))\n",
    "\n",
    "\n",
    "    num_data = len(X)\n",
    "    num_train = int(0.8 * num_data)\n",
    "    X_train = torch.tensor(X[:num_train])\n",
    "    Y_train = torch.tensor(Y[:num_train])\n",
    "\n",
    "\n",
    "    batch_size = 64\n",
    "    num_batches = int(len(X_train) / batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        print(epoch)\n",
    "        epoch_loss = 0\n",
    "        for i in range(num_batches):\n",
    "            X_batch = X_train[i * batch_size:(i + 1) * batch_size].float().to(device)\n",
    "            Y_batch = Y_train[i * batch_size:(i + 1) * batch_size].to(device)\n",
    "            # print(X_batch)\n",
    "            # print(Y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(my_model(X_batch), Y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += float(loss)\n",
    "        print('train loss : ', float(epoch_loss / num_batches))\n",
    "\n",
    "\n",
    "\n",
    "train()\n",
    "\n",
    "print(\"Training finished\")\n",
    "\n",
    "\n",
    "for param_tensor in my_model.state_dict():\n",
    "    print(param_tensor, \"\\t\", my_model.state_dict()[param_tensor].size())\n",
    "torch.save(my_model.state_dict(), 'bangali_embeddings_model')\n",
    "\n",
    "data = {}\n",
    "data['tweets'] = new_tweets\n",
    "data['Y'] = np.array(labels)\n",
    "data['all_words'] = np.array(V)\n",
    "save_data(data, 'bangali_tweets')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
