{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this script I do the final task which is classification of the tweets. The script can be used for both Hindi and Bangali datasets by changing the dataset_name. The CNN that I use is the one developed in https://arxiv.org/pdf/1408.5882.pdf for text classification. It consists of 300 filters on each of the convolutional layers which have filter sizes of [3,4,5] respectively. It also consists of a 50% dropout layer and maxpooling layers on top of each convolution layer. ReLU activation is used to add non-linearity after each convolution layer. I used the implementation of this CNN provided in https://github.com/clinicalml/learn-to-defer/blob/master/language/hatespeech_defer.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from utils import *\n",
    "import sys\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim,\n",
    "                 dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        # self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        self.conv_0 = nn.Conv2d(in_channels=1,\n",
    "                                out_channels=n_filters,\n",
    "                                kernel_size=(filter_sizes[0], embedding_dim))\n",
    "\n",
    "        self.conv_1 = nn.Conv2d(in_channels=1,\n",
    "                                out_channels=n_filters,\n",
    "                                kernel_size=(filter_sizes[1], embedding_dim))\n",
    "\n",
    "        self.conv_2 = nn.Conv2d(in_channels=1,\n",
    "                                out_channels=n_filters,\n",
    "                                kernel_size=(filter_sizes[2], embedding_dim))\n",
    "\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.logsoftmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, embedded):\n",
    "        # text = [batch size, sent len]\n",
    "\n",
    "        # embedded = self.embedding(text)\n",
    "\n",
    "        # embedded = [batch size, sent len, emb dim]\n",
    "\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        # print(embedded.shape)\n",
    "\n",
    "        # embedded = [batch size, 1, sent len, emb dim]\n",
    "\n",
    "        conved_0 = F.relu(self.conv_0(embedded).squeeze(3))\n",
    "        # print(conved_0.shape)\n",
    "        conved_1 = F.relu(self.conv_1(embedded).squeeze(3))\n",
    "        # print(conved_1.shape)\n",
    "        conved_2 = F.relu(self.conv_2(embedded).squeeze(3))\n",
    "        # print(conved_2.shape)\n",
    "\n",
    "        # conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "\n",
    "        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n",
    "        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n",
    "        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n",
    "\n",
    "        # pooled_n = [batch size, n_filters]\n",
    "\n",
    "        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1))\n",
    "\n",
    "        # cat = [batch size, n_filters * len(filter_sizes)]\n",
    "\n",
    "        fc =  self.fc(cat)\n",
    "        return self.logsoftmax(fc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train function. We use Negative Log Likelihood as the loss function and Adam as our optimizer. The validation loss is also reported at the end of each epoch. Based on the validation loss I tuned the hyper parameters such as learning rate, number of epochs etc. The model which has the best performance on the validation set is saved as the final model. By looking at training and validation loss, it gets clear that the model is overfitting, since the training loss is decreasing while validation loss is not. I tried to overcome this by reducing the number of filters (so using less parameters) and tuning other hyperparameters which was not really successful. The final test accuracy on hindi dataset is 72%.\n",
    "Note that for the bangali dataset we start from the model which was trained on hindi dataset and fine tune that model on the bangali dataset rather than training from scratch. The testing accuracy on Bangali dataset is 69%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_file):\n",
    "    data = load_data(data_file)\n",
    "    X = data['X'].to(device)\n",
    "    print(X.shape)\n",
    "    Y = data['Y'].to(device)\n",
    "    print(torch.sum(Y))\n",
    "    val_X = data['val']['X'].to(device)\n",
    "    val_Y = data['val']['Y'].to(device)\n",
    "    print(torch.sum(val_Y))\n",
    "\n",
    "    batch_size = 32\n",
    "    num_batches = int(X.shape[0]/batch_size)\n",
    "\n",
    "    N_FILTERS = 100  # hyperparameterr\n",
    "    FILTER_SIZES = [3, 4, 5]\n",
    "    DROPOUT = 0.5\n",
    "    INPUT_DIM = data['X'].shape[1]\n",
    "    EMBEDDING_DIM = 300\n",
    "    OUTPUT_DIM = 2\n",
    "    model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT).to(device)\n",
    "    #finetune\n",
    "    if dataset_name == 'bangali':\n",
    "        model.load_state_dict(torch.load('hindi_classifier_model'))\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr = 0.005)\n",
    "    loss_func = torch.nn.NLLLoss()\n",
    "\n",
    "    num_epochs = 5\n",
    "    min_val_loss = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        print(epoch)\n",
    "        epoch_loss = 0\n",
    "        for i in range(num_batches):\n",
    "            X_batch = X[i * batch_size:(i+1) * batch_size]\n",
    "            Y_batch = Y[i * batch_size:(i + 1) * batch_size].long()\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_func(model(X_batch),Y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += float(loss)\n",
    "        print('train_loss : ',epoch_loss/num_batches)\n",
    "        with torch.no_grad():\n",
    "            val_loss = loss_func(model(val_X),val_Y.long())\n",
    "            if val_loss < min_val_loss:\n",
    "                torch.save(model.state_dict(), dataset_name + '_classifier_model')\n",
    "                min_val_loss = val_loss\n",
    "\n",
    "        print('validation_loss : ' , float(val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing function. The model is loaded and the prediction of the model is compared with ground truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test(data_file):\n",
    "    data = load_data(data_file)\n",
    "    X = data['test']['X'].to(device)\n",
    "    Y = data['test']['Y'].to(device)\n",
    "    N_FILTERS = 100  # hyperparameterr\n",
    "    FILTER_SIZES = [3, 4, 5]\n",
    "    DROPOUT = 0\n",
    "    INPUT_DIM = data['X'].shape[1]\n",
    "    EMBEDDING_DIM = 300\n",
    "    OUTPUT_DIM = 2\n",
    "    model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT).to(device)\n",
    "    model.load_state_dict(torch.load(dataset_name + '_classifier_model'))\n",
    "    with torch.no_grad():\n",
    "        pred = torch.argmax(model(X),dim=1)\n",
    "        \n",
    "    print('test accuracy : ', np.mean(np.array([p==y for p,y in zip(pred.cpu().data.numpy(),Y.cpu().data.numpy())])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3732, 25, 300])\n",
      "tensor(1973)\n",
      "tensor(246)\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-a78afc671f55>:58: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.logsoftmax(fc)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss :  0.7749652857410496\n",
      "validation_loss :  0.6482155919075012\n",
      "1\n",
      "train_loss :  0.6208056419573981\n",
      "validation_loss :  0.6262603402137756\n",
      "2\n",
      "train_loss :  0.5668973133995615\n",
      "validation_loss :  0.6184963583946228\n",
      "3\n",
      "train_loss :  0.5247747594940251\n",
      "validation_loss :  0.678963840007782\n",
      "4\n",
      "train_loss :  0.5097625859338661\n",
      "validation_loss :  0.6138694882392883\n",
      "test accuracy :  0.723175965665236\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'hindi' #change to bingali\n",
    "data_file = dataset_name + '_data'\n",
    "train(data_file)\n",
    "test(data_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
