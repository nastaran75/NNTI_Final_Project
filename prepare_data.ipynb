{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this script I get the embeddings and prepare the data so that it can be directly fed to the CNN later on. The script can be used for both Bangali and hindi datasets by only changing the dataset name in the beginning of the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import csv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from utils import *\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the cleaned data which was saved in the previous script as well as the list of the vocabulary items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'hindi' #change to bangali\n",
    "embedding_size = 300\n",
    "data = load_data(dataset_name + '_tweets')\n",
    "V = data['all_words']\n",
    "new_tweets = data['tweets']\n",
    "Y = data['Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word2vec model. Here in the forward pass I do not use the second layer and the final softmax layer since I only want to get the embeddings which are in the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.hidden_1 = nn.Linear(len(V),embedding_size)\n",
    "    self.hidden_2 = nn.Linear(embedding_size,len(V))\n",
    "    self.logsoftmax = nn.LogSoftmax()\n",
    "\n",
    "\n",
    "  def forward(self, one_hot):\n",
    "    out = self.hidden_1(one_hot)\n",
    "    # print(out.shape)\n",
    "    # out = self.hidden_2(out)\n",
    "    # out = self.logsoftmax(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the function which turns a word into its one hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_one_hot(word):\n",
    "    one_hot = torch.zeros(len(V))\n",
    "    for i,w in enumerate(V):\n",
    "        if word == w:\n",
    "            one_hot[i] = 1\n",
    "            break\n",
    "    assert(torch.sum(one_hot)==1)\n",
    "    return one_hot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I load the embeddings model which we trained in the previous script and define a sen_len which is the mean of the sentence length of the sentences in the dataset. since for the CNN we need all the sentences to have the same size but the sentences in the dataset can have various sizes. I use the mean of the length as the final length of all the sentences and crop the longer sentences and pad the shorter sentences so that they all satisfy this length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec()\n",
    "model.load_state_dict(torch.load(dataset_name + '_embeddings_model'))\n",
    "model.eval()\n",
    "sen_len = int(sum([len(sentence) for sentence in new_tweets])/len(new_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each word in each sentence, we first get its one hot representation based on the vocaulary and then use the embeddings model to get a feature vector for that word. Note that for each sentence we will have a sen_len * 300 feature matrix since the word2vec models gives us 300 features for each word and we decided to use sen_len as the length of all tweets. For the tweets which are shorter than sen_len, the remaining values are filled with random number as in https://arxiv.org/pdf/1408.5882.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = torch.zeros((len(new_tweets),sen_len,embedding_size))\n",
    "# Y = torch.zeros((len(new_tweets)))\n",
    "for i,sentence in enumerate(new_tweets):\n",
    "    print(i)\n",
    "    j = 0\n",
    "    cnt = 0\n",
    "    for word in sentence:\n",
    "        if j == sen_len:\n",
    "            break\n",
    "        if word in V:\n",
    "            embeddings[i][j] = model(word_to_one_hot(word))\n",
    "            cnt += 1\n",
    "        else:\n",
    "            print('-----ERR------')\n",
    "        j += 1\n",
    "#     print(sen_len,len(sentence), cnt)\n",
    "    while j<sen_len:\n",
    "        embeddings[i][j] = torch.rand(embedding_size)\n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we split the data into train, validation and test set and save it as a dictionary so that we can later it load it for our classification task. 80% of the data is used for training, 10% for validation and 10% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(268)\n"
     ]
    }
   ],
   "source": [
    "Y = torch.from_numpy(Y)\n",
    "print(torch.sum(Y))\n",
    "import random\n",
    "frac = 0.8\n",
    "num_data = len(new_tweets)\n",
    "num_train = int(frac * num_data)\n",
    "num_test = int((num_data - num_train)/2)\n",
    "num_val = num_data - (num_test + num_train)\n",
    "indices = np.arange(num_data)\n",
    "random.shuffle(indices)\n",
    "indices_train = indices[:num_train]\n",
    "indices_val = indices[num_train:num_train+num_val]\n",
    "indices_test = indices[num_train+num_val:num_train+num_val+num_test]\n",
    "\n",
    "data_split = {}\n",
    "data_split['X'] = embeddings[indices_train]\n",
    "data_split['Y'] = Y[indices_train]\n",
    "\n",
    "\n",
    "val = {}\n",
    "val['X'] = embeddings[indices_val]\n",
    "val['Y'] = Y[indices_val]\n",
    "\n",
    "data_split['val'] = val\n",
    "\n",
    "test = {}\n",
    "test['X'] = embeddings[indices_test]\n",
    "test['Y'] = Y[indices_test]\n",
    "data_split['test'] = test\n",
    "\n",
    "save_data(data_split,dataset_name + '_data')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
